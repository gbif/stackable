apiVersion: hdfs.stackable.tech/v1alpha1
kind: HdfsCluster
metadata:
  name: {{ include "gbif-chart-lib.name" . }}
spec:
  image:
    productVersion: {{ default .Chart.AppVersion .Values.stackProduct }}
    stackableVersion: {{ default "23.1.0" .Values.stackVersion }}
  clusterConfig:
    zookeeperConfigMapName: {{ .Values.zookeeperClusterName }}-znode
    dfsReplication: {{ .Values.dataReplication }}
{{- if .Values.logging.enabled  }}     
    vectorAggregatorConfigMapName: {{ .Values.logging.discoveryMap }}
{{- end }}
  nameNodes:
    roleGroups:
      default:
        replicas: {{ .Values.nodes.namenode.replicas }}
        config:
          resources:
            cpu:
              max: {{ .Values.nodes.namenode.cpu.max }}
              min: {{ .Values.nodes.namenode.cpu.min }}
            storage:
              data:
                capacity: {{ .Values.nodes.namenode.storage.capacity }}
            memory:
              limit: {{ .Values.nodes.namenode.memory }}
{{- if .Values.logging.enabled  }}
          logging:
            enableVectorAgent: true
            containers:
              vector:
                file:
                  level: {{ .Values.logging.vectorLogLevel }}
              hdfs:
                console:
                  level: {{ .Values.nodes.namenode.logLevel }}
                file:
                  level: {{ .Values.nodes.namenode.logLevel }}
                loggers:
                  ROOT:
                    level: {{ .Values.nodes.namenode.logLevel }}
{{- end }}
{{- if eq .Values.useNodeAffinity true}}
          affinity:
            nodeAffinity: 
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: namenodes
                    operator: In
                    values:
                    - enabled
{{- end }}
        configOverrides:
          hdfs-site.xml:
            dfs.client.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            dfs.datanode.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            nfs.superuser: "{{ .Values.gateway.nfs.user }}"
            nfs.dump.dir: "{{ .Values.gateway.nfs.localDirectory }}"
            nfs.exports.allowed.hosts: "{{ .Values.gateway.nfs.exportAccess }}"
            nfs.export.point: "{{ .Values.gateway.nfs.directoryToExport }}"
          core-site.xml:
            hadoop.proxyuser.stackable.hosts: "{{ .Values.gateway.httpfs.proxyHosts }}"
            hadoop.proxyuser.stackable.groups: "{{ .Values.gateway.httpfs.proxyGroup }}"
  dataNodes:
    config:
{{- if .Values.logging.enabled  }}
      logging:
        enableVectorAgent: true
        containers:
          vector:
            file:
              level: {{ .Values.logging.vectorLogLevel }}
          hdfs:
            console:
              level: {{ .Values.nodes.datanode.logLevel }}
            file:
              level: {{ .Values.nodes.datanode.logLevel }}
            loggers:
              ROOT:
                level: {{ .Values.nodes.datanode.logLevel }}
{{- end }}
{{- if eq .Values.useNodeAffinity true }}
      affinity:
        nodeAffinity: 
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: datanodes
                operator: In
                values:
                - enabled
{{- end }}
    configOverrides:
      hdfs-site.xml:
        dfs.client.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
        dfs.datanode.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
        nfs.superuser: "{{ .Values.gateway.nfs.user }}"
        nfs.dump.dir: "{{ .Values.gateway.nfs.localDirectory }}"
        nfs.exports.allowed.hosts: "{{ .Values.gateway.nfs.exportAccess }}"
        nfs.export.point: "{{ .Values.gateway.nfs.directoryToExport }}"
      core-site.xml:
        hadoop.proxyuser.stackable.hosts: "{{ .Values.gateway.httpfs.proxyHosts }}"
        hadoop.proxyuser.stackable.groups: "{{ .Values.gateway.httpfs.proxyGroup }}"
    roleGroups:
{{- range $datanodeKey, $datanodeValue := .Values.nodes.datanode.groups }}
      {{ $datanodeKey }}:
        replicas: {{ $datanodeValue.replicas }}
        config:
          resources:
            cpu:
              max: {{ $datanodeValue.cpu.max }}
              min: {{ $datanodeValue.cpu.min }}
            memory:
              limit: {{ $datanodeValue.memory }}
            {{- if gt (len $datanodeValue.storage.drives) 0 }}
            storage:
              data:
                count: 0 # Setting count to 0 disables the default PVCs
              disks:
                count: {{ len $datanodeValue.storage.drives }}
                capacity: {{ $datanodeValue.storage.capacity }} # Currently the same for all disks but in the future it could easily be different for different types of disks
                storageClass: {{ $datanodeValue.storage.class }}
                hdfsStorageType: {{ $datanodeValue.storage.type }}
            {{- else }}
            storage:
              data:
                capacity: {{ $datanodeValue.storage.capacity }}
            {{- end }}
{{- end }}
  journalNodes:
    roleGroups:
      default:
        replicas: {{ .Values.nodes.journalnode.replicas }}
        config:
          resources:
            cpu:
              max: {{ .Values.nodes.journalnode.cpu.max }}
              min: {{ .Values.nodes.journalnode.cpu.min }}
            memory:
              limit: {{ .Values.nodes.journalnode.memory }}
            storage:
              data:
                capacity: {{ .Values.nodes.journalnode.storage.capacity }}
{{- if .Values.logging.enabled  }}
          logging:
            enableVectorAgent: true
            containers:
              vector:
                file:
                  level: {{ .Values.logging.vectorLogLevel }}
              hdfs:
                console:
                  level: {{ .Values.nodes.journalnode.logLevel }}
                file:
                  level: {{ .Values.nodes.journalnode.logLevel }}
                loggers:
                  ROOT:
                    level: {{ .Values.nodes.journalnode.logLevel }}
{{- end }}
        configOverrides:
          hdfs-site.xml:
            dfs.client.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            dfs.datanode.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            nfs.superuser: "{{ .Values.gateway.nfs.user }}"
            nfs.dump.dir: "{{ .Values.gateway.nfs.localDirectory }}"
            nfs.exports.allowed.hosts: "{{ .Values.gateway.nfs.exportAccess }}"
            nfs.export.point: "{{ .Values.gateway.nfs.directoryToExport }}"
          core-site.xml:
            hadoop.proxyuser.stackable.hosts: "{{ .Values.gateway.httpfs.proxyHosts }}"
            hadoop.proxyuser.stackable.groups: "{{ .Values.gateway.httpfs.proxyGroup }}"
