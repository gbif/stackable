apiVersion: hdfs.stackable.tech/v1alpha1
kind: HdfsCluster
metadata:
  name: {{ include "gbif-chart-lib.name" . }}
spec:
  image:
    productVersion: {{ default .Chart.AppVersion .Values.stackProduct }}
    stackableVersion: {{ default "23.1.0" .Values.stackVersion }}
  clusterConfig:
    zookeeperConfigMapName: {{ .Values.zookeeperClusterName }}-znode
    dfsReplication: {{ .Values.dataReplication }}
  nameNodes:
    roleGroups:
      default:
        replicas: {{ .Values.nodes.namenode.replicas }}
        config:
          resources:
            cpu:
              max: {{ .Values.nodes.namenode.cpu.max }}
              min: {{ .Values.nodes.namenode.cpu.min }}
            storage:
              data:
                capacity: {{ .Values.nodes.namenode.storage.capacity }}
            memory:
              limit: {{ .Values.nodes.namenode.memory }}
          {{- if eq .Values.useNodeAffinity true}}
          affinity:
            nodeAffinity: 
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: namenodes
                    operator: In
                    values:
                    - enabled
          {{- end }}
        configOverrides:
          hdfs-site.xml:
            dfs.client.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            dfs.datanode.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            nfs.superuser: "{{ .Values.gateway.nfs.user }}"
            nfs.dump.dir: "{{ .Values.gateway.nfs.localDirectory }}"
            nfs.exports.allowed.hosts: "{{ .Values.gateway.nfs.exportAccess }}"
            nfs.export.point: "{{ .Values.gateway.nfs.directoryToExport }}"
          core-site.xml:
            hadoop.proxyuser.stackable.hosts: "{{ .Values.gateway.httpfs.proxyHosts }}"
            hadoop.proxyuser.stackable.groups: "{{ .Values.gateway.httpfs.proxyGroup }}"
  dataNodes:
    roleGroups:
      default:
        replicas: {{ .Values.nodes.datanode.replicas }}
        config:
          resources:
            cpu:
              max: {{ .Values.nodes.datanode.cpu.max }}
              min: {{ .Values.nodes.datanode.cpu.min }}
            memory:
              limit: {{ .Values.nodes.datanode.memory }}
            {{- if gt (len .Values.nodes.datanode.storage.drives) 0 }}
            storage:
              data:
                count: 0 # Setting count to 0 disables the default PVCs
              hard-disks:
                count: {{ len .Values.nodes.datanode.storage.drives }}
                capacity: {{ .Values.nodes.datanode.storage.capacity }} # Currently the same for all disks but in the future it could easily be different for different types of disks
                storageClass: {{ .Values.nodes.datanode.storage.class }}
                hdfsStorageType: {{ .Values.nodes.datanode.storage.type }}
            {{- else }}
            storage:
              data:
                capacity: {{ .Values.nodes.datanode.storage.capacity }}
            {{- end }}
          {{- if eq .Values.useNodeAffinity true }}
          affinity:
            nodeAffinity: 
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: datanodes
                    operator: In
                    values:
                    - enabled
          {{- end }}
        configOverrides:
          hdfs-site.xml:
            dfs.client.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            dfs.datanode.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            nfs.superuser: "{{ .Values.gateway.nfs.user }}"
            nfs.dump.dir: "{{ .Values.gateway.nfs.localDirectory }}"
            nfs.exports.allowed.hosts: "{{ .Values.gateway.nfs.exportAccess }}"
            nfs.export.point: "{{ .Values.gateway.nfs.directoryToExport }}"
          core-site.xml:
            hadoop.proxyuser.stackable.hosts: "{{ .Values.gateway.httpfs.proxyHosts }}"
            hadoop.proxyuser.stackable.groups: "{{ .Values.gateway.httpfs.proxyGroup }}"
  journalNodes:
    roleGroups:
      default:
        replicas: {{ .Values.nodes.journalnode.replicas }}
        config:
          resources:
            cpu:
              max: {{ .Values.nodes.journalnode.cpu.max }}
              min: {{ .Values.nodes.journalnode.cpu.min }}
            memory:
              limit: {{ .Values.nodes.journalnode.memory }}
            storage:
              data:
                capacity: {{ .Values.nodes.journalnode.storage.capacity }}
        configOverrides:
          hdfs-site.xml:
            dfs.client.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            dfs.datanode.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            nfs.superuser: "{{ .Values.gateway.nfs.user }}"
            nfs.dump.dir: "{{ .Values.gateway.nfs.localDirectory }}"
            nfs.exports.allowed.hosts: "{{ .Values.gateway.nfs.exportAccess }}"
            nfs.export.point: "{{ .Values.gateway.nfs.directoryToExport }}"
          core-site.xml:
            hadoop.proxyuser.stackable.hosts: "{{ .Values.gateway.httpfs.proxyHosts }}"
            hadoop.proxyuser.stackable.groups: "{{ .Values.gateway.httpfs.proxyGroup }}"
