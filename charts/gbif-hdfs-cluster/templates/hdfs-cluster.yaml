apiVersion: hdfs.stackable.tech/v1alpha1
kind: HdfsCluster
metadata:
  name: {{ include "gbif-chart-lib.name" . }}
  namespace: {{ .Release.Namespace }}
spec:
  image:
{{ include "gbif-chart-lib.stackableImage" . | indent 4 }}
  clusterOperation:
    reconciliationPaused: {{ .Values.clusterOperation.reconciliationPaused }}
    stopped: {{ .Values.clusterOperation.stopped }}
  clusterConfig:
    zookeeperConfigMapName: {{ .Values.zookeeperClusterName }}-znode
    dfsReplication: {{ .Values.dataReplication }}
{{- if .Values.logging.enabled  }}     
    vectorAggregatorConfigMapName: {{ .Values.logging.discoveryMap }}
{{- end }}
  nameNodes:
    config:
      listenerClass: {{ .Values.listenerClass }}
{{- if (or .Values.nodes.namenode.nodeAffinity .Values.nodes.namenode.podAntiAffinity)}}
      affinity:
{{- with .Values.nodes.namenode.nodeAffinity }}
        nodeAffinity:
{{- toYaml . | nindent 10 }}
{{- end }}
{{- with .Values.nodes.namenode.podAntiAffinity }}
        podAntiAffinity:
{{- toYaml . | nindent 10 }}
{{- end }}
{{- end }}
{{- if .Values.yunikorn.enabled }}
    podOverrides:
      metadata:
        labels:
{{- include "gbif-chart-lib.yunikornLabels" . | nindent 10 }}
{{- end }}
    roleGroups:
      default:
        replicas: {{ .Values.nodes.namenode.replicas }}
        config:
          resources:
            cpu:
              max: {{ .Values.nodes.namenode.cpu.max }}
              min: {{ .Values.nodes.namenode.cpu.min }}
            storage:
              data:
                capacity: {{ .Values.nodes.namenode.storage.capacity }}
            memory:
              limit: {{ .Values.nodes.namenode.memory }}
{{- if .Values.logging.enabled  }}
          logging:
            enableVectorAgent: true
            containers:
              vector:
                file:
                  level: {{ .Values.logging.vectorLogLevel }}
              hdfs:
                console:
                  level: {{ .Values.nodes.namenode.logLevel }}
                file:
                  level: {{ .Values.nodes.namenode.logLevel }}
                loggers:
                  ROOT:
                    level: {{ .Values.nodes.namenode.logLevel }}
{{- end }}
        configOverrides:
          hdfs-site.xml:
            dfs.client.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            dfs.datanode.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            nfs.superuser: "{{ .Values.gateway.nfs.user }}"
            nfs.dump.dir: "{{ .Values.gateway.nfs.localDirectory }}"
            nfs.exports.allowed.hosts: "{{ .Values.gateway.nfs.exportAccess }}"
            nfs.export.point: "{{ .Values.gateway.nfs.directoryToExport }}"
          core-site.xml:
            hadoop.proxyuser.stackable.hosts: "{{ .Values.gateway.httpfs.proxyHosts }}"
            hadoop.proxyuser.stackable.groups: "{{ .Values.gateway.httpfs.proxyGroup }}"
  dataNodes:
{{- if .Values.yunikorn.enabled }}
    podOverrides:
      metadata:
        labels:
{{- include "gbif-chart-lib.yunikornLabels" . | nindent 10 }}
{{- end }}
    config:
{{- if .Values.logging.enabled  }}
      logging:
        enableVectorAgent: true
        containers:
          vector:
            file:
              level: {{ .Values.logging.vectorLogLevel }}
          hdfs:
            console:
              level: {{ .Values.nodes.datanode.logLevel }}
            file:
              level: {{ .Values.nodes.datanode.logLevel }}
            loggers:
              ROOT:
                level: {{ .Values.nodes.datanode.logLevel }}
{{- end }}
{{- if (or .Values.nodes.datanode.nodeAffinity .Values.nodes.datanode.podAntiAffinity)}}
      affinity:
{{- with .Values.nodes.datanode.nodeAffinity }}
        nodeAffinity:
{{- toYaml . | nindent 10 }}
{{- end }}
{{- with .Values.nodes.datanode.podAntiAffinity }}
        podAntiAffinity:
{{- toYaml . | nindent 10 }}
{{- end }}
{{- end }}
    configOverrides:
      hdfs-site.xml:
        dfs.client.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
        dfs.datanode.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
        nfs.superuser: "{{ .Values.gateway.nfs.user }}"
        nfs.dump.dir: "{{ .Values.gateway.nfs.localDirectory }}"
        nfs.exports.allowed.hosts: "{{ .Values.gateway.nfs.exportAccess }}"
        nfs.export.point: "{{ .Values.gateway.nfs.directoryToExport }}"
      core-site.xml:
        hadoop.proxyuser.stackable.hosts: "{{ .Values.gateway.httpfs.proxyHosts }}"
        hadoop.proxyuser.stackable.groups: "{{ .Values.gateway.httpfs.proxyGroup }}"
    roleGroups:
{{- range $datanodeKey, $datanodeValue := .Values.nodes.datanode.groups }}
      {{ $datanodeKey }}:
        replicas: {{ $datanodeValue.replicas }}
        config:
          resources:
            cpu:
              max: {{ $datanodeValue.cpu.max }}
              min: {{ $datanodeValue.cpu.min }}
            memory:
              limit: {{ $datanodeValue.memory }}
            {{- if gt (len $datanodeValue.storage.drives) 0 }}
            storage:
              data:
                count: 0 # Setting count to 0 disables the default PVCs
              disks:
                count: {{ len $datanodeValue.storage.drives }}
                capacity: {{ $datanodeValue.storage.capacity }} # Currently the same for all disks but in the future it could easily be different for different types of disks
                storageClass: {{ $datanodeValue.storage.class }}
                hdfsStorageType: {{ $datanodeValue.storage.type }}
            {{- else }}
            storage:
              data:
                capacity: {{ $datanodeValue.storage.capacity }}
            {{- end }}
{{- end }}
  journalNodes:
{{- if .Values.yunikorn.enabled }}
    podOverrides:
      metadata:
        labels:
{{- include "gbif-chart-lib.yunikornLabels" . | nindent 10 }}
{{- end }}
    roleGroups:
      default:
        replicas: {{ .Values.nodes.journalnode.replicas }}
        config:
          resources:
            cpu:
              max: {{ .Values.nodes.journalnode.cpu.max }}
              min: {{ .Values.nodes.journalnode.cpu.min }}
            memory:
              limit: {{ .Values.nodes.journalnode.memory }}
            storage:
              data:
                capacity: {{ .Values.nodes.journalnode.storage.capacity }}
{{- if .Values.logging.enabled  }}
          logging:
            enableVectorAgent: true
            containers:
              vector:
                file:
                  level: {{ .Values.logging.vectorLogLevel }}
              hdfs:
                console:
                  level: {{ .Values.nodes.journalnode.logLevel }}
                file:
                  level: {{ .Values.nodes.journalnode.logLevel }}
                loggers:
                  ROOT:
                    level: {{ .Values.nodes.journalnode.logLevel }}
{{- end }}
        configOverrides:
          hdfs-site.xml:
            dfs.client.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            dfs.datanode.use.datanode.hostname: "{{ .Values.useDataNodeHostName }}"
            nfs.superuser: "{{ .Values.gateway.nfs.user }}"
            nfs.dump.dir: "{{ .Values.gateway.nfs.localDirectory }}"
            nfs.exports.allowed.hosts: "{{ .Values.gateway.nfs.exportAccess }}"
            nfs.export.point: "{{ .Values.gateway.nfs.directoryToExport }}"
          core-site.xml:
            hadoop.proxyuser.stackable.hosts: "{{ .Values.gateway.httpfs.proxyHosts }}"
            hadoop.proxyuser.stackable.groups: "{{ .Values.gateway.httpfs.proxyGroup }}"
