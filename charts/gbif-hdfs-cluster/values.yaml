# Default values for GBIF HDFS cluster using Stackable HDFS operator.

# Override name of the cluster
nameOverride: ""
# Override fullname of the cluster
fullnameOverride: ""

# Stackable versioning to override the version of the Stack component.
stackProduct: "3.3.4"
stackVersion: "24.3.0"

# Uses the stackProduct value to tell the operator what version of configurations to use with the custom image
# custom image allows customer to change the repository if they want to their own acting as a proxy or
# use an extended version of the image
# Example of how to set custom image
# customImage: 
#  enabled: true
#  repository: docker.gbif.org/your-team
#  extended:
#    enabled: true
#    image: a-extended-stackable-image
#    tag: "1.0.5"
customImage:
  enabled: false

# Name of the zookeeper cluster
zookeeperClusterName: "gbif-zookeeper"

# Number of times data should be replicated in the HDFS cluster
dataReplication: 2

# Value for enabling node affinity.
# A specific label is used on the nodes to indicate if namenode and/or datanode should be running on the node
# It is done in case you need a specific range of servers to expose the nodes outside the cluster
# For making node available for nodename, add label namenodes=enabled to the node
# For making node available for datanode, add label datanodes=enabled to the node
useNodeAffinity: true

# Enable logging via the vector aggregator
# Needs the name of configmap for discovery of the aggregator
logging:
  # property for enable vector logging
  enabled: false
  # DisoveryMap for the connection the aggregator pod
  discoveryMap: "gbif-vector-aggregator-discovery"
  # Configured which vector itself logs on its own.
  vectorLogLevel: "WARN"

# Note describing configurations for NFS and HTTPFS to propergate to the hadoop cluster
gateway:
  httpfs:
    # Hosts allowed to proxy through the webhdfs
    proxyHosts: "*"
    # Groups allowed to proxy through the webhdfs
    proxyGroup: "*"
  nfs:
    # ACL user to acces the system
    user: "stackable"
    # Local directory to use a intermidiate storage
    localDirectory: "/home/stackable/data"
    # Directory within the HDFS cluster to be exported
    directoryToExport: "/occurrence-download/some-directory"
    # Hosts and their access to the system
    exportAccess: "* ro"

# Support for Yunikorn queuing
# By default it is disabled.
# The idea with enabling yunikorn for components is that we can fence the infrastructure components from works done in Spark, Trino etc.
yunikorn:
  enabled: false
  # Name of the overall application for all the pods within the queue
  appId: "hdfs"
  # Queue to place the pods in
  queue: "root.namespace.infra"

nodes:
  datanode:
    logLevel: "INFO"
    # Allow for specifying nodeports for exposing the pods to VMs outside the kubenetes cluster on the internal network
    # ListenerClass for the datanode
    listenerClass: "cluster-internal"
    groups:
      default:
        replicas: 1
        cpu:
          min: '100m'
          max: '1000m'
        memory: '2Gi'
        # The storage structure allows for defining node specific drives created by the cluster administrator
        # If you want to let the cluster deploy the storage, you can do the following:
        # storage:
        #   capacity: '50Gi'
        storage:
          drives: []
          labels: []
          class: hdfs-datanode-storage
          capacity: '100Gi'
          type: "Disk"
      # Example of how to add an additional group with a different type of disk
      #ssdmachines:
      #  replicas: 1
      #  cpu:
      #    min: '100m'
      #    max: '2000m'
      #  memory: '4Gi'
      #  # The storage structure allows for defining node specific drives created by the cluster administrator
      #  # If you want to let the cluster deploy the storage, you can do the following:
      #  # storage:
      #  #   capacity: '50Gi'
      #  storage:
      #    drives: []
      #    labels: []
      #    class: hdfs-datanode-ssd-storage
      #    capacity: '300Gi'
      #    type: "SSD"

    # Example of how nodeAffinity could look
    # nodeAffinity: 
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: datanodes
    #         operator: In
    #         values:
    #         - enabled
    nodeAffinity: {}
    # Example of how podAntiAffinity could look
    # podAntiAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #   - labelSelector:
    #       matchLabels:
    #         app.kubernetes.io/name: hdfs
    #         app.kubernetes.io/instance: gbif-hdfs-cluster
    #         app.kubernetes.io/component: datanode
    #     topologyKey: kubernetes.io/hostname
    podAntiAffinity: {}
  namenode:
    replicas: 1
    cpu:
      min: '100m'
      max: '2000m'
    memory: '3Gi'
    logLevel: "INFO"
    storage:
      capacity: '15Gi'
    # ListenerClass for the namenode
    listenerClass: "cluster-internal"
    # Example of how nodeAffinity could look
    # nodeAffinity: 
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: namenodes
    #         operator: In
    #         values:
    #         - enabled
    nodeAffinity: {}
    # Example of how podAntiAffinity could look
    # podAntiAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #   - labelSelector:
    #       matchLabels:
    #         app.kubernetes.io/name: hdfs
    #         app.kubernetes.io/instance: gbif-hdfs-cluster
    #         app.kubernetes.io/component: namenode
    #     topologyKey: kubernetes.io/hostname
    podAntiAffinity: {}
  journalnode:
    replicas: 3
    cpu:
      min: '100m'
      max: '2000m'
    memory: '2Gi'
    logLevel: "INFO"
    storage:
      capacity: '15Gi'

clusterOperation:
  reconciliationPaused: false
  stopped: false

# Set what the HDFS operator should use to create the topology used for Hadoop rack awareness
# Example
# rackAwareness:
#   - nodeLabel: topology.kubernetes.io/zones
#   - podLabel: app.kubernetes.io/role-group
rackAwareness: []