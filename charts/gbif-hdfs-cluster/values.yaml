# Default values for GBIF HDFS cluster using Stackable HDFS operator.

# Override name of the cluster
nameOverride: ""
# Override fullname of the cluster
fullnameOverride: ""

# Stackable versioning to override the version of the Stack component.
stackProduct: "3.3.4"
stackVersion: "23.7.0"

# Name of the zookeeper cluster
zookeeperClusterName: "gbif-zookeeper"

# Number of times data should be replicated in the HDFS cluster
dataReplication: 2

# Controls if the datanode should use hostname to resolve each others addresses. 
# Used so clients outside the cluster can resolve the IPs locally instead of the internal IPs of the Kubernetes cluster.
useDataNodeHostName: "true"

# Value for enabling node affinity.
# A specific label is used on the nodes to indicate if namenode and/or datanode should be running on the node
# It is done in case you need a specific range of servers to expose the nodes outside the cluster
# For making node available for nodename, add label namenodes=enabled to the node
# For making node available for datanode, add label datanodes=enabled to the node
useNodeAffinity: true

# Note describing configurations for NFS and HTTPFS to propergate to the hadoop cluster
gateway:
  httpfs:
    # Hosts allowed to proxy through the webhdfs
    proxyHosts: "*"
    # Groups allowed to proxy through the webhdfs
    proxyGroup: "*"
  nfs:
    # ACL user to acces the system
    user: "stackable"
    # Local directory to use a intermidiate storage
    localDirectory: "/home/stackable/data"
    # Directory within the HDFS cluster to be exported
    directoryToExport: "/occurrence-download/some-directory"
    # Hosts and their access to the system
    exportAccess: "* ro"
nodes:
  datanode:
    replicas: 1
    cpu:
      min: '100m'
      max: '1000m'
    memory: '2Gi'
    # The storage structure allows for defining node specific drives created by the cluster administrator
    # If you want to let the cluster deploy the storage, you can do the following:
    # storage:
    #   capacity: '50Gi'
    storage:
      drives: []
      labels: []
      class: hdfs-datanode-storage
      capacity: '100Gi'
      type: "Disk"
    # Allow for specifying nodeports for exposing the pods to VMs outside the kubenetes cluster on the internal network
    tcpPort: 9864
    icpPort: 9867
    dataPort: 9866
    # List of IPs hosting the specific nodes.
    # Example server1 host datanode2 and server2 host datanode1
    # Then it should look like:
    # ips:
    #  - server2_ip
    #  - server1_ip
    ips: []
  namenode:
    replicas: 1
    cpu:
      min: '100m'
      max: '2000m'
    memory: '3Gi'
    storage:
      capacity: '15Gi'
    # Allow for specifying nodeports for exposing the pods to VMs outside the kubenetes cluster on the internal network
    tcpPort: 9870
    rcpPort: 8020
    # List of IPs hosting the specific nodes.
    # Example server1 host namenode2 and server2 host namenodenode1
    # Then it should look like:
    # ips:
    #  - server2_ip
    #  - server1_ip
    ips: []
  journalnode:
    replicas: 3
    cpu:
      min: '100m'
      max: '2000m'
    memory: '2Gi'
    storage:
      capacity: '15Gi'
    # Currently no support for external port in our service template for journalnode exposure
    ips: []