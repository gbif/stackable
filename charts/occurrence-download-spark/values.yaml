# Default values for GBIF Spark application for process data and prepare map tiles.
# Name of the application, chart uses the name to find the .jar file.
appName: "occurrence-download-spark"

# Override name of the cluster
nameOverride: ""
# Override fullname of the cluster
fullnameOverride: ""

# Repository, image name and version for finding the image containing the application jar
# Example
# image:
#  repository: "test.docker.org/a_folder"
#  #The template assumes the name of the jar is the same as the images name
#  name: "MySupercoolImage"
#  version: "1.0.0"
image: 
  repository: "docker.gbif.org/"
  name: "gridded-datasets"
  version: "0.195.0-H3-SNAPSHOT"
  alwaysPull: false

# Stackable versioning to override the version of the Stack component.
stackProduct: "3.5.0"
stackVersion: "23.7.0"

# Uses the stackProduct value to tell the operator what version of configurations to use with the custom image
# custom image allows customer to change the repository if they want to their own acting as a proxy or
# use an extended version of the image
# Example of how to set custom image
# customImage: 
#  enabled: true
#  repository: docker.gbif.org/your-team
#  extended:
#    enabled: true
#    image: a-extended-stackable-image
#    tag: "1.0.5"
customImage: {}

# Spark running mode, currently only cluster is supported.
mode: cluster
# Main class for starting the Spark process.
mainClass: org.gbif.occurrence.download.spark.SparkDownloads

# The list of arguments to pass to the spark process
# Example
#args:
#  - my_first_argument
#  - my_second_argument
args:
  - "config.yaml"
# deps describes the dependencies the spark-submit pod should pull before starting execution
# the section consists on a list of repositories and a list of packages
# Example 
#deps:
#  repositories:
#    - "https://some_maven_repo/repository/central/"
#  packages:
#    - "org.apache.spark:spark-avro_2.12:3.3.0"
deps:
  repositories:
    - https://repository.gbif.org/repository/central/
  packages:
    - "org.apache.spark:spark-avro_2.12:3.3.0"

# sparkConf is used to pass spark configuration to spark submitter, driver and executor
# Example of how it could look
# sparkConf:
#  "spark.driver.extraClassPath": "/etc/hadoop/conf/:/etc/gbif/:/stackable/spark/extra-jars/*"
#  "spark.executor.extraClassPath": "/etc/hadoop/conf/:/etc/gbif/:/stackable/spark/extra-jars/*"
#  "spark.kubernetes.authenticate.driver.serviceAccountName": "gbif-spark-sa"
#  "spark.kubernetes.scheduler.name": "yunikorn"
#  "spark.kubernetes.driver.label.queue": "root.default"
#  "spark.kubernetes.executor.label.queue": "root.default"
#  "spark.kubernetes.driver.annotation.yunikorn.apache.org/app-id": "{{`{{APP_ID}}`}}"
#  "spark.kubernetes.executor.annotation.yunikorn.apache.org/app-id": "{{`{{APP_ID}}`}}"

sparkConf: {}

# Configuration for a bucket to store logs in
# Supports one existing bucket
# Example of how it could be configured
#bucket:
#  name: test-bucket
#  # IMPORTANT NOTE
#  # The prefix must eixsts the history server can start properly
#  prefix: sparkjobs-logs/
#  # S3 connection configurations
#  connection:
#    host: a-test-minio
#    port: 9000
#    # Name of the spark history installation in the cluster
#    sparkHistoryName: "a-spark-history-server"

bucket: {}

# Enable logging via the vector aggregator
# Needs the name of configmap for discovery of the aggregator
logging:
  # property for enable vector logging
  enabled: false
  # DisoveryMap for the connection the aggregator pod
  discoveryMap: "gbif-vector-aggregator-discovery"
  # Configured which vector itself logs on its own.
  vectorLogLevel: "WARN"

# Ressource configuration for spark driver.
nodes:
  driver:
    # CPU settings for the driver 
    cpu: 
      min: "1000m"
      max: "2000m"
    # Memory for the Spark driver.
    memory: "2Gi"
    # Log level for driver
    logLevel: "INFO"

  # Ressource configuration for spark executors.
  executor:
    # CPU settings for per executor. 
    # To get total reosurce consumtion by executors, multiple max cpi and mem with number of replicas.
    cpu:
      min: "1000m" 
      max: "6000m"
    # Memory for each executor.
    memory: "10Gi"
    # Number of executors.
    replicas: 2
    # Log level for executor
    logLevel: "INFO"

#Mode for installing. This is a implemtantion for selecting which objects to install.
## all: Installs both spark application and configmaps - will fail if configmap already exists
## config: Installs the configmaps maps into a namespace
## app: Installs the spark application
# The use case is for enabling templating only the application for use in Airflows or start an instance of spark within necessary updating the configsmaps
installMode: config

#Support for Yunikorn scheduling together with dynamic resource allocation with the help from yunikorn.
#By default both options are disabled.
yunikorn:
  enabled: false
  user: "a-spark-user"
  queue: "root.default"
  appId: "spark"
  dynamicResources:
    enabled: false
    executor:
      initial: 2
      min: 1
      max: 6

# As the chart assumes a default structure for providing properties to the application it is possible to configure a way to provide your own format
# Example of how to describe you own file (You have to create this configmap yourself for now)
# customProperties:
#  configmapName: "my-cool-configmap"
#  path: "/some/path/on/pod/"
#  file: "my-super-cool-file.properties"

customProperties: 
  configmapName: "occurrence-download-spark-conf"
  path: "/stackable/spark/jobs"
  file: "download.properties"

# Application configs
apiUrl: ""
coreTermName: Occurrence
createTablesDynamically: false
es:
  connectTimeout: 7000
  hosts: ""
  index:
    nested: false
    type: OCCURRENCE
    name: ""
  socketTimeout: 120000
hbaseTable: occurrence
hdfs:
  lock:
    connection:
      maxRetries: 5
      sleepTimeMs: 100
    name: hdfsview
    namespace: ""
    path: /hive/
  namenode: ""
hive:
  db: ""
  metastoreUris: ""
  server2: ""
  user: ""
  warehouseDir: ""
occurrence:
  download:
    bionomiaReducer:
      memory: ""
      opts: ""
    fileMaxRecords: ""
    hdfsTmpDir: /tmp/
    hdfsOutputPath: ""
    hiveHdfsOut: ""
    job:
      maxThreads: 3
      minRecords: 200
    link: ""
    maxConnectionPool: 10
    maxGlobalThreads: 10
    tmpDir: /tmp/
    ws:
      password: ""
      username: ""
    zookeeper:
      downloadsNamespace: ""
      indicesNamespace: ""
      lockName: occDownloadJobsCounter
      maxRetries: 10
      sleepTime: 1000
  envPrefix: ""
  environment: ""
  hdfsBuildFrequency: ${coord\:days(1)}
registryWsUrl: ""
synctables: false
tableName: occurrence
wfPrefix: occurrence
zkConnectionString: ""
