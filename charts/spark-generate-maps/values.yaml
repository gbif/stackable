# Default values for GBIF Spark application for process data and prepare map tiles.
# Name of the application, chart uses the name to find the .jar file.
appName: "spark-generate-maps"

# Override name of the cluster
nameOverride: ""
# Override fullname of the cluster
fullnameOverride: ""

# Repository, image name and version for finding the image containing the application jar
# Example
# image:
#  repository: "test.docker.org/a_folder"
#  #The template assumes the name of the jar is the same as the images name
#  name: "MySupercoolImage"
#  version: "1.0.0"
image: 
  repository: "docker.gbif.org/"
  name: "spark-generate-maps"
  version: "0.37.0-H3-SNAPSHOT"
  alwaysPull: false

# Stackable versioning to override the version of the Stack component.
stackProduct: "3.4.0"
stackVersion: "23.11.0"

# Uses the stackProduct value to tell the operator what version of configurations to use with the custom image
# custom image allows customer to change the repository if they want to their own acting as a proxy or
# use an extended version of the image
# Example of how to set custom image
# customImage: 
#  enabled: true
#  repository: docker.gbif.org/your-team
#  extended:
#    enabled: true
#    image: a-extended-stackable-image
#    tag: "1.0.5"
customImage: {}

# Spark running mode, currently only cluster is supported.
mode: cluster
# Main class for starting the Spark process.
mainClass: org.gbif.maps.workflow.Backfill

# The list of arguments to pass to the spark process
# Example
#args:
#  - my_first_argument
#  - my_second_argument
args: 
  - "tiles"
  - "config.yaml"
# deps describes the dependencies the spark-submit pod should pull before starting execution
# the section consists on a list of repositories and a list of packages
# Example 
#deps:
#  repositories:
#    - "https://some_maven_repo/repository/central/"
#  packages:
#    - "org.apache.spark:spark-avro_2.12:3.3.0"
# sparkConf is used to pass spark configuration to spark submitter, driver and executor
# Example of how it could look
# sparkConf:
#  "spark.driver.extraClassPath": "/etc/hadoop/conf/:/etc/gbif/:/stackable/spark/extra-jars/*"
#  "spark.executor.extraClassPath": "/etc/hadoop/conf/:/etc/gbif/:/stackable/spark/extra-jars/*"
#  "spark.kubernetes.authenticate.driver.serviceAccountName": "gbif-spark-sa"
#  "spark.kubernetes.scheduler.name": "yunikorn"
#  "spark.kubernetes.driver.label.queue": "root.default"
#  "spark.kubernetes.executor.label.queue": "root.default"
#  "spark.kubernetes.driver.annotation.yunikorn.apache.org/app-id": "{{`{{APP_ID}}`}}"
#  "spark.kubernetes.executor.annotation.yunikorn.apache.org/app-id": "{{`{{APP_ID}}`}}"

sparkConf: {}

# Configuration for a bucket to store logs in
# Supports one existing bucket
# Example of how it could be configured
#bucket:
#  name: test-bucket
#  # IMPORTANT NOTE
#  # The prefix must eixsts the history server can start properly
#  prefix: sparkjobs-logs/
#  # S3 connection configurations
#  connection:
#    host: a-test-minio
#    port: 9000
#    # Name of the spark history installation in the cluster
#    sparkHistoryName: "a-spark-history-server"

bucket: {}

# Enable logging via the vector aggregator
# Needs the name of configmap for discovery of the aggregator
logging:
  # property for enable vector logging
  enabled: false
  # DisoveryMap for the connection the aggregator pod
  discoveryMap: "gbif-vector-aggregator-discovery"
  # Configured which vector itself logs on its own.
  vectorLogLevel: "WARN"

# Ressource configuration for spark driver.
nodes:
  driver:
    # CPU settings for the driver 
    cpu: 
      min: "1000m"
      max: "2000m"
    # Memory for the Spark driver.
    memory: "2Gi"
    # Log level for driver
    logLevel: "INFO"

  # Ressource configuration for spark executors.
  executor:
    # CPU settings for per executor. 
    # To get total reosurce consumtion by executors, multiple max cpi and mem with number of replicas.
    cpu:
      min: "1000m" 
      max: "6000m"
    # Memory for each executor.
    memory: "8Gi"
    # Number of executors.
    replicas: 2
    # Log level for executor
    logLevel: "INFO"

#Mode for installing. This is a implemtantion for selecting which objects to install.
## all: Installs both spark application and configmaps - will fail if configmap already exists
## config: Installs the configmaps maps into a namespace
## app: Installs the spark application
# The use case is for enabling templating only the application for use in Airflows or start an instance of spark within necessary updating the configsmaps
installMode: config

#Support for Yunikorn scheduling together with dynamic resource allocation with the help from yunikorn.
#By default both options are disabled.
yunikorn:
  enabled: false
  user: "a-spark-user"
  queue: "root.default"
  appId: "spark"
  dynamicResources:
    enabled: false
    executor:
      initial: 2
      min: 1
      max: 6

# As the chart assumes a default structure for providing properties to the application it is possible to configure a way to provide your own format
# Example of how to describe you own file (You have to create this configmap yourself for now)
# customProperties:
#  configmapName: "my-cool-configmap"
#  path: "/some/path/on/pod/"
#  file: "my-super-cool-file.properties"

customProperties: {}

hdfs:
  # Name of the Stackable HdfsCluster in the namespace.
  clusterName: gbif-hdfs
  # Directory of the snapshot location, needs to be created beforehand in the HDFS cluster.
  snapshotDir: data/hdfsview/occurrence/
  # Source directory of the data loaded into the HDFS cluster.
  sourceSubDir: occurrence/
  # Target directory for the produced HFiles from the Spark application. Currently needs to be cleaned afeter each run.
  targetDir: tmp/hfiles/dev_maps
  # Space in zookeeper to place the lock
  namespace: dev_index_lock
  # Path to place the lock
  lockPath: /hive/
  # Lock name
  lockName: hdfsview
  # Timeout time for each try, set in ms.
  sleepMs: 100
  # Number of retries
  retries: 5

map:
  # Number of minimum entries before it is created as a tile
  tileThreshold: 1000
  # Tile buffer size
  tileBuffer: 64
  # Tile size
  tileSize: 512
  # zoom levels goes from 0 up to limit underneath
  zoom: 16
  # Name of Zookeper node to write metadata for the table
  metadataPath: "/dev2_maps/meta"

hive:
  # Name of the hive cluster
  clusterName: gbif-hive-metastore
  # Name of the hive database
  schema: dev

hbase:
  # Name of the hbase cluster
  clusterName: gbif-hbase
  # Table name in HBase, needs to be created first with the correct number of regions
  tableName: dev_maps
  # Modulus for doing the salts in hbase
  saltModulus: 10