# Default values for GBIF airflow cluster using Stackable airflow operator.

# Override name of the cluster
nameOverride: ""
# Override fullname of the cluster
fullnameOverride: ""

# Stackable versioning to override the version of the Stack component.
stackProduct: "2.8.1"
stackVersion: "24.3.0"

# Uses the stackProduct value to tell the operator what version of configurations to use with the custom image
# custom image allows customer to change the repository if they want to their own acting as a proxy or
# use an extended version of the image
# Example of how to set custom image
# customImage: 
#  enabled: true
#  repository: docker.gbif.org/your-team
#  extended:
#    enabled: true
#    image: a-extended-stackable-image
#    tag: "1.0.5"
customImage:
  enabled: false

# Should load examples
examples: false

# Expose configs
exposeConfigs: false

# If configured, will use git to sync dags to the pods
# Example of how to configure git
# git: 
#   repository: https://github.com/my_org/my_repo
#   branch: main
#   dagFolder: root_of_repo/my/dag/folder
#   # size of the history the repo should keep locally
#   depth: 10
#   # wait time between syncs
#   wait: 30
#   # If auth is defined a create will be created with provided info that gitsync will use to checkout repository
#   auth:
#     user: a_github_user
#     token: a_token
#     

git: {}

# Admin user for Airflow, will be created as a secret
admin:
  firstName: ""
  lastName: ""
  email: ""
  user: ""
  password: ""
  secret: ""

# Database information for Airflows state database
airflowDB:
  name: ""
  user: ""
  pass: ""
  db: airflow

# Redis information for Airflows queue system
airflowRedis:
  name: airflow-redis
  pass: ""

# Enable logging via the vector aggregator
# Needs the name of configmap for discovery of the aggregator
logging:
  # property for enable vector logging
  enabled: false
  # DisoveryMap for the connection the aggregator pod
  discoveryMap: "gbif-vector-aggregator-discovery"
  # Configured which vector itself logs on its own.
  vectorLogLevel: "WARN"

# Support for Yunikorn queuing
# By default it is disabled.
# The idea with enabling yunikorn for components is that we can fence the infrastructure components from works done in Spark, Trino etc.
yunikorn:
  enabled: false
  # Name of the overall application for all the pods within the queue
  appId: "airflow"
  # Queue to place the pods in
  queue: "root.namespace.infra"

ingress:
  enabled: false
  # Domain to create the ingress entry with
  domain: example.org

nodes:
  webserver:
    replicas: 1
    cpu:
      min: '100m'
      max: '2000m'
    memory: '2Gi'
    logLevel: "INFO"
    # Port for setting up node port given node - Leave empty to not setup port
    ports: {}
  workers:
    replicas: 1
    cpu:
      min: '100m'
      max: '3000m'
    memory: '4Gi'
    logLevel: "INFO"
    # Port for setting up node port given node - Leave empty to not setup port
    ports: {}
  schedulers:
    replicas: 1
    cpu:
      min: '100m'
      max: '1000m'
    memory: '2Gi'
    logLevel: "INFO"
    # Port for setting up node port given node - Leave empty to not setup port
    ports: {}

# Example of how the airflowEnvOverrides can look like. Every line gets added to env override for all type of nodes
# NOTE overwriting the plugin or dag location should only be done if you have made the required changes to airflowCluster.yaml file 
# airflowEnvOverrides:
#  AIRFLOW__CORE__DAGS_FOLDER: "{{ .Values.externalData }}/airflow/dags"
#  AIRFLOW__CORE__PLUGINS_FOLDER: "{{ .Values.externalData }}/airflow/plugins"
#  AIRFLOW__CORE__EXECUTE_TASKS_NEW_PYTHON_INTERPRETER: "True"
#  AIRFLOW_CONN_KUBERNETES_IN_CLUSTER: "kubernetes://?__extra__=%7B%22extra__kubernetes__in_cluster%22%3A+true%2C+%22extra__kubernetes__kube_config%22%3A+%22%22%2C+%22extra__kubernetes__kube_config_path%22%3A+%22%22%2C+%22extra__kubernetes__namespace%22%3A+%22%22%7D"

airflowEnvOverrides: {}

clusterOperation:
  reconciliationPaused: false
  stopped: false