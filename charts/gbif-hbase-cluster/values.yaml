# Default values for GBIF HDFS cluster using Stackable HDFS operator.

# Override name of the cluster
nameOverride: ""
# Override fullname of the cluster
fullnameOverride: ""

# Stackable tag to override the version of the Stack component.
stackProduct: "2.4.12"
stackVersion: "23.11.0"

# Uses the stackProduct value to tell the operator what version of configurations to use with the custom image
# custom image allows customer to change the repository if they want to their own acting as a proxy or
# use an extended version of the image
# Example of how to set custom image
# customImage: 
#  enabled: true
#  repository: docker.gbif.org/your-team
#  extended:
#    enabled: true
#    image: a-extended-stackable-image
#    tag: "1.0.5"
customImage: {}

# Name of the zookeeper cluster
zookeeperClusterName: "gbif-zookeeper"

# Name of the HDFS cluster
hdfsClusterName: "gbif-hdfs"

# hbase root folder for storing data
hbaseRootDirectory: /hbase

# Enable logging via the vector aggregator
# Needs the name of configmap for discovery of the aggregator
logging:
  # property for enable vector logging
  enabled: false
  # DisoveryMap for the connection the aggregator pod
  discoveryMap: "gbif-vector-aggregator-discovery"
  # Configured which vector itself logs on its own.
  vectorLogLevel: "WARN"

# Support for Yunikorn queuing
# By default it is disabled.
# The idea with enabling yunikorn for components is that we can fence the infrastructure components from works done in Spark, Trino etc.
yunikorn:
  enabled: true
  # Name of the overall application for all the pods within the queue
  appId: "hbase"
  # Queue to place the pods in
  queue: "root.namespace.infra"

nodes:
  master:
    replicas: 1
    cpu:
      min: '100m'
      max: '2000m'
    memory: '4Gi'
    logLevel: "INFO"
    storage:
      capacity: '4Gi'
    # Currently no support in the service template for external ip ports for master node
    ips: []
  region:
    replicas: 3
    cpu:
      min: '100m'
      max: '3000m'
    memory: '8Gi'
    logLevel: "INFO"
    storage:
      capacity: '16Gi'
    tcpPort: 16020
    # List of IPs hosting the specific nodes.
    # Example server1 host region2 and server2 host region1
    # Then it should look like:
    # ips:
    #  - server2_ip
    #  - server1_ip
    ips: []
  rest:
    replicas: 1
    cpu:
      min: '100m'
      max: '1000m'
    memory: '2Gi'
    logLevel: "INFO"
    storage:
      capacity: '2Gi'
    # Currently no support in the service template for external ip ports for rest node
    ips: []