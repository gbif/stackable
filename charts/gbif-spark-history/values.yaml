# Default values for GBIF spark history server using Stackable Spark operator.

# Override name of the cluster
nameOverride: ""
# Override fullname of the cluster
fullnameOverride: ""

# Stackable versioning to override the version of the Stack component.
stackProduct: "3.5.1"
stackVersion: "24.3.0"

# Uses the stackProduct value to tell the operator what version of configurations to use with the custom image
# custom image allows customer to change the repository if they want to their own acting as a proxy or
# use an extended version of the image
# Example of how to set custom image
# customImage: 
#  enabled: true
#  repository: docker.gbif.org/your-team
#  extended:
#    enabled: true
#    image: a-extended-stackable-image
#    tag: "1.0.5"
customImage:
  enabled: false

# Configuration for a bucket to store logs in
# Supports exactly one existing bucket - no more, no less
bucket:
  name: test-bucket
  # IMPORTANT NOTE
  # The prefix must eixsts the history server can start properly 
  prefix: sparkjobs-logs/
  # S3 connection configurations
  connection:
    host: a-test-minio
    port: 9000
    credentials:
      # Ensure the user has read&write access for the bucket
      user: a_user
      password: a_password

# Support for Yunikorn queuing
# By default it is disabled.
# The idea with enabling yunikorn for components is that we can fence the infrastructure components from works done in Spark, Trino etc.
yunikorn:
  enabled: false
  # Name of the overall application for all the pods within the queue
  appId: "spark-history"
  # Queue to place the pods in
  queue: "root.namespace.infra"

ingress:
  enabled: false
  # Domain to create the ingress entry with
  domain: example.org

nodes:
  historyServer:
    replicas: 1
    cpu:
      min: '100m'
      max: '1500m'
    memory: 2Gi

# Used to pass spark configuration
# Below there is an example on how to do it
#sparkConf:
#  "spark.hadoop.fs.s3a.path.style.access": "true"
#  "spark.com.amazonaws.services.s3.enableV4": "true"
#  "spark.hadoop.com.amazonaws.services.s3.enableV4": "true"
#  "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"

sparkConf: {}