# Default values for GBIF Spark application for process data and prepare map tiles.
# Name of the application, chart uses the name to find the .jar file.
appName: ""
# Repository and tag for finding the image if selfhosted.
image:
  repository: ""
  tag: ""

# Stackable tag to use for the Spark application.
sparkTag: 3.3.0-stackable0.1.0
# Spark running mode, currently only cluster is supported.
mode: cluster
# Main class for starting the Spark process.
appClass: org.gbif.maps.spark.Backfill

# Ressource configuration for spark driver.
driver:
  # Number of cores for the Spark driver.
  core: 2
  # Memory for the Spark driver.
  mem: 2048m

# Ressource configuration for spark executors.
executor:
  # Number of cores for each executor.
  core: 8
  # Memory for each executor.
  mem: 30000m
  # Number of executors.
  instances: 3

nameOverride: ""
fullnameOverride: ""

hdfs:
  # Name of the Stackabl HdfsCluster in the namespace.
  clusterName: gbif-hdfs
  # Directory of the snapshot location, needs to be created beforehand in the HDFS cluster.
  snapshotDir: data/hdfs/hdfsview/occurrence/
  # Source directory of the data loaded into the HDFS cluster.
  sourceSubDir: occurrence/
  # Target directory for the produced HFiles from the Spark application. Currently needs to be cleaned afeter each run.
  targetDir: temp/dev_maps
  # Space in zookeeper to place the lock
  namespace: KUBE_index
  # Path to place the lock
  lockPath: /hive/
  # Lock name
  lockName: hdfsview
  # Timeout time for each try, set in ms.
  sleepMs: 100
  # Number of retries
  retries: 5

zookeeper:
  # Quorum needs to point to the quorum string. Take from discovery configmap for your Stackable zookeeper cluster in the desired namespace.
  quorum: default-zookeeper-default-0.default-namespace.svc.cluster.local:2282
  # Path to the root directory in zookeeper. Take from discovery map for your Stackable zookeeper cluster in the desired namespace.
  root: /

# Number of minimum entries before it is created as a tile
tileThreshold: 1000

hbase:
  # Number of regions in Hbase
  regions: 10

points:
  # Table name in HBase, needs to be created first with the correct number of regions
  tableName: KUBE
  # Number of partitions to process data in
  sparkPartitions: 200
  # Number of HFiles to create
  numberOfHFiles: 32

tiles:
  # Table name in HBase, needs to be created first with the correct number of regions
  tableName: KUBE
  # Number of partitions to process data in
  sparkPartitions: 200
  # Number of HFiles to create
  numberOfHFiles: 100
  tileBuffer: 64
  #Add entries for which maps to process in the spark job.
  typeOfMaps: 
    EPSG_3857:
      min: 0
      max: 16
      tileSize: 512
      srs: "EPSG:3857"